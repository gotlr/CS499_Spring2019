for (iter.index in (1:max.iterations)) {
grad <- t(X.scale.vec) %*% (X.scale.vec %*% W -  y.vec);
if (sqrt(as.numeric(t(grad) %*% grad)) < 1e-6){
println(sprintf('iter times=%d', iter.index));
break;
}
W <- W - grad * step.size;
W.mat[,iter.index]=W
}
return(W.mat)
}
LMLogisticLossIterations <-
function(X.mat, y.vec,max.iterations, step.size = 0.05) {
if (!all(is.matrix(X.mat), is.numeric(X.mat))) {
stop("X.mat must be a numeric matrix.")
}
if (!all(is.vector(y.vec),
is.numeric(y.vec),
length(y.vec) == nrow(X.mat))) {
stop("y.vec must be a numeric vector of the same number of rows as X.mat.")
}
if (!all(is.integer(max.iterations),
max.iterations > 1,
length(max.iterations) == 1)) {
stop("Input max.iterations must be a greater than 1 integer scalar number.")
}
if (!all(is.numeric(step.size), length(step.size) == 1)) {
stop("step.size must be a numeric scalar value.")
}
num.train <- dim(X.mat)[1]
num.feature <- dim(X.mat)[2]
W=matrix(data=0,ncol=1,nrow=num.feature)
sigmoid <- function(z) { 1 / (1 + exp(-z))}
X.scale.vec <- scale(X.mat,center=TRUE,scale = TRUE)[1:num.train,1:num.feature]#variance=1,mean=0
W.mat=matrix(0,ncol=max.iterations,nrow=num.feature)
for (i in 1:max.iterations){
grad <- t(X.scale.vec) %*% (sigmoid(X.scale.vec %*% W)-y.vec);
if (sqrt(as.numeric(t(grad) %*% grad)) < 1e-6){
print(sprintf('iter times=%d', i));
break;``
}
W <- W - grad * step.size;
W.mat[,i]=W
}
return(W.mat)
}
LMSquareLossIterations <-
function(X.mat, y.vec, max.iterations, step.size = 0.001) {
if (!all(is.matrix(X.mat), is.numeric(X.mat))) {
stop("X.mat must be a numeric matrix.")
}
if (!all(is.vector(y.vec),
is.numeric(y.vec),
length(y.vec) == nrow(X.mat))) {
stop("y.vec must be a numeric vector of the same number of rows as X.mat.")
}
if (!all(is.integer(max.iterations),
max.iterations > 1,
length(max.iterations) == 1)) {
stop("Input max.iterations must be a greater than 1 integer scalar number.")
}
if (!all(is.numeric(step.size), length(step.size) == 1)) {
stop("step.size must be a numeric scalar value.")
}
num.train <- dim(X.mat)[1]
num.feature <- dim(X.mat)[2]
W=matrix(data=0,ncol=1,nrow=num.feature)
W.mat=matrix(0,ncol=max.iterations,nrow=num.feature)
X.scale.vec <- scale(X.mat,center=TRUE,scale = TRUE)[1:num.train,1:num.feature]#variance=1,mean=0
# for-loop to get the slope.mat matrix
for (iter.index in (1:max.iterations)) {
grad <- t(X.scale.vec) %*% (X.scale.vec %*% W -  y.vec);
if (sqrt(as.numeric(t(grad) %*% grad)) < 1e-6){
println(sprintf('iter times=%d', iter.index));
break;
}
W <- W - grad * step.size;
W.mat[,iter.index]=W
}
return(W.mat)
}
LMLogisticLossIterations <-
function(X.mat, y.vec,max.iterations, step.size = 0.05) {
if (!all(is.matrix(X.mat), is.numeric(X.mat))) {
stop("X.mat must be a numeric matrix.")
}
if (!all(is.vector(y.vec),
is.numeric(y.vec),
length(y.vec) == nrow(X.mat))) {
stop("y.vec must be a numeric vector of the same number of rows as X.mat.")
}
if (!all(is.integer(max.iterations),
max.iterations > 1,
length(max.iterations) == 1)) {
stop("Input max.iterations must be a greater than 1 integer scalar number.")
}
if (!all(is.numeric(step.size), length(step.size) == 1)) {
stop("step.size must be a numeric scalar value.")
}
num.train <- dim(X.mat)[1]
num.feature <- dim(X.mat)[2]
W=matrix(data=0,ncol=1,nrow=num.feature)
sigmoid <- function(z) { 1 / (1 + exp(-z))}
X.scale.vec <- scale(X.mat,center=TRUE,scale = TRUE)[1:num.train,1:num.feature]#variance=1,mean=0
W.mat=matrix(0,ncol=max.iterations,nrow=num.feature)
for (i in 1:max.iterations){
grad <- t(X.scale.vec) %*% (sigmoid(X.scale.vec %*% W)-y.vec);
if (sqrt(as.numeric(t(grad) %*% grad)) < 1e-6){
print(sprintf('iter times=%d', i));
break;``
}
W <- W - grad * step.size;
W.mat[,i]=W
}
return(W.mat)
}
LMLogisticLossIterations <-
function(X.mat, y.vec,max.iterations, step.size = 0.05) {
if (!all(is.matrix(X.mat), is.numeric(X.mat))) {
stop("X.mat must be a numeric matrix.")
}
if (!all(is.vector(y.vec),
is.numeric(y.vec),
length(y.vec) == nrow(X.mat))) {
stop("y.vec must be a numeric vector of the same number of rows as X.mat.")
}
if (!all(is.integer(max.iterations),
max.iterations > 1,
length(max.iterations) == 1)) {
stop("Input max.iterations must be a greater than 1 integer scalar number.")
}
if (!all(is.numeric(step.size), length(step.size) == 1)) {
stop("step.size must be a numeric scalar value.")
}
num.train <- dim(X.mat)[1]
num.feature <- dim(X.mat)[2]
W=matrix(data=0,ncol=1,nrow=num.feature)
sigmoid <- function(z) { 1 / (1 + exp(-z))}
X.scale.vec <- scale(X.mat,center=TRUE,scale = TRUE)[1:num.train,1:num.feature]#variance=1,mean=0
W.mat=matrix(0,ncol=max.iterations,nrow=num.feature)
for (i in 1:max.iterations){
grad <- t(X.scale.vec) %*% (sigmoid(X.scale.vec %*% W)-y.vec);
if (sqrt(as.numeric(t(grad) %*% grad)) < 1e-6){
print(sprintf('iter times=%d', i));
break;
}
W <- W - grad * step.size;
W.mat[,i]=W
}
return(W.mat)
}
LMLogisticLossIterations <-
function(X.mat, y.vec,max.iterations, step.size = 0.05) {
if (!all(is.matrix(X.mat), is.numeric(X.mat))) {
stop("X.mat must be a numeric matrix.")
}
if (!all(is.vector(y.vec),
is.numeric(y.vec),
length(y.vec) == nrow(X.mat))) {
stop("y.vec must be a numeric vector of the same number of rows as X.mat.")
}
if (!all(is.integer(max.iterations),
max.iterations > 1,
length(max.iterations) == 1)) {
stop("Input max.iterations must be a greater than 1 integer scalar number.")
}
if (!all(is.numeric(step.size), length(step.size) == 1)) {
stop("step.size must be a numeric scalar value.")
}
num.train <- dim(X.mat)[1]
num.feature <- dim(X.mat)[2]
W=matrix(data=0,ncol=1,nrow=num.feature)
sigmoid <- function(z) { 1 / (1 + exp(-z))}
X.scale.vec <- scale(X.mat,center=TRUE,scale = TRUE)[1:num.train,1:num.feature]#variance=1,mean=0
W.mat=matrix(0,ncol=max.iterations,nrow=num.feature)
for (i in 1:max.iterations){
grad <- t(X.scale.vec) %*% (sigmoid(X.scale.vec %*% W)-y.vec);
if (sqrt(as.numeric(t(grad) %*% grad)) < 1e-6){
print(sprintf('iter times=%d', i));
break;
}
W <- W - grad * step.size;
W.mat[,i]=W
}
return(W.mat)
}
Rscript linear_models.R
LMLogisticLossIterations <-
function(X.mat, y.vec,max.iterations, step.size = 0.05) {
if (!all(is.matrix(X.mat), is.numeric(X.mat))) {
stop("X.mat must be a numeric matrix.")
}
if (!all(is.vector(y.vec),
is.numeric(y.vec),
length(y.vec) == nrow(X.mat))) {
stop("y.vec must be a numeric vector of the same number of rows as X.mat.")
}
if (!all(is.integer(max.iterations),
max.iterations > 1,
length(max.iterations) == 1)) {
stop("Input max.iterations must be a greater than 1 integer scalar number.")
}
if (!all(is.numeric(step.size), length(step.size) == 1)) {
stop("step.size must be a numeric scalar value.")
}
num.train <- dim(X.mat)[1]
num.feature <- dim(X.mat)[2]
W=matrix(data=0,ncol=1,nrow=num.feature)
sigmoid <- function(z) { 1 / (1 + exp(-z))}
X.scale.vec <- scale(X.mat,center=TRUE,scale = TRUE)[1:num.train,1:num.feature]#variance=1,mean=0
W.mat=matrix(0,ncol=max.iterations,nrow=num.feature)
for (i in 1:max.iterations){
grad <- t(X.scale.vec) %*% (sigmoid(X.scale.vec %*% W)-y.vec);
if (sqrt(as.numeric(t(grad) %*% grad)) < 1e-6){
print(sprintf('iter times=%d', i));
break;
}
W <- W - grad * step.size;
W.mat[,i]=W
}
return(W.mat)
}
LMLogisticLossEarlyStoppingCV<-function(X.mat, y.vec, fold.vec,folds.n=2,max.iterations)
{
step.size = 0.01
#give the initial step size
Data = cbind(X.mat ,y.vec)
#Randomly shuffle the data
Data<-Data[sample(nrow(Data)),]
#split the data into several folds
Start = 0
End   = NCOL(Data) - 1
LabelCol      = NCOL(Data)
Rows          = NROW(Data)
#initialize Error Matrix:
L2Error.Matrix = 0
#Perform folds.n fold cross validation
for(i in 1:folds.n){
#Segement your data by fold using the which() function
positions <- which(fold.vec == i)
testdata <- Data[positions, ]
traindata <- Data[-positions, ]
test.data    <- testdata[,DataStart:DataEnd]
train.labels  <- testdata[,LabelCol]
train.data   <- trainData[,Start:End]
train.labels <- trainData[,LabelCol]
#use LMLogisticLossIterations to compute a sequence of models
W= LMSquareLossIterations(train.data,train.labels,max.iterations,step.size)
#on the train data, then compute the validation loss of each model.
Error = 0
Y.sigmoid = 1/(1+exp(-data.matrix(TestingData) %*% data.matrix(W.Matrix)))
Error_Vector = ifelse(Y.sigmoid>0.5,1,0) != as.integer(TestingLables)
Error = (as.matrix(colMeans(Error_Vector)))
LMLogisticLoss.Matrix   <- rbind( LMLogisticLoss.Matrix,t(Error))
}
#compute mean.validation.loss.vec, which is a vector (with max.iterations elements) of mean validation loss over all K folds.
mean.validation.loss = colMeans( LMLogisticLoss.Matrix)
#minimize the mean validation loss to determine selected.steps, the optimal number of steps/iterations.
SmallestLoss = max(a)
selected.steps = 0
for(i in 1:max.iterations)
{
if(mean.validation.loss[i] < SmallestLoss)
{
SmallestLoss = mean.validation.loss[i]
selected.steps        = i
}
}
#finally use LMLinearSquareLossIterations(max.iterations=selected.steps) on the whole training data set.
weight.vec = LMLogisticLossIterations(X.mat, y.vec,selected.steps,step.size)
#try to get the train.loss.vec(I don't know what does training loss mean)
#predict(testX.mat)
Y.sigmoid = 1/(1+exp(-data.matrix(X.mat) %*% data.matrix(weight.vec)))
testX.mat=ifelse(Y.sigmoid>0.5,1,0) != as.integer(TestingLables)
#return list
list=list(mean.validation.loss,selected.steps,weight.vec,testX.mat)
return (list)
}
LMLogisticLossEarlyStoppingCV<-function(X.mat, y.vec, fold.vec,folds.n=2,max.iterations)
{
step.size = 0.01#give the initial step size
Data = cbind(X.mat ,y.vec)
#Randomly shuffle the data
Data<-Data[sample(nrow(Data)),]
#TODO If invalid fold.vec .. make one for ourselves
Start = 0
End   = NCOL(Data) - 1
LabelCol      = NCOL(Data)
Rows          = NROW(Data)
#initialize Error Matrix:
L2Error.Matrix = 0
#Perform folds.n fold cross validation
for(i in 1:folds.n){
#Segement your data by fold using the which() function
positions <- which(fold.vec == i)
testdata <- Data[positions, ]
traindata <- Data[-positions, ]
test.data    <- testdata[,Start:End]
train.labels  <- testdata[,LabelCol]
train.data   <- trainData[,DataColsStart:DataColsEnd]
train.labels <- trainData[,LabelCol]
#use LMLogisticLossIterations to compute a sequence of models
W= LMSquareLossIterations(train.data,train.labels,max.iterations,step.size)
#on the train data, then compute the validation loss of each model.
Error = 0
Y.square = data.matrix(TestingData) %*% data.matrix(W)
Error_Vector = ((Y.square) - as.integer(TestingLables))**2
Error = (as.matrix(colMeans(Error_Vector)))
LMLogisticLoss.Matrix   <- rbind( LMLogisticLoss.Matrix,t(Error))
}
#compute mean.validation.loss.vec, which is a vector (with max.iterations elements) of mean validation loss over all K folds.
mean.validation.loss = colMeans( LMLogisticLoss.Matrix)
#minimize the mean validation loss to determine selected.steps, the optimal number of steps/iterations.
SmallestLoss = max(a)
selected.steps = 0
for(i in 1:max.iterations)
{
if(mean.validation.loss[i] < SmallestLoss)
{
SmallestLoss = mean.validation.loss[i]
selected.steps        = i
}
}
#finally use LMLinearSquareLossIterations(max.iterations=selected.steps) on the whole training data set.
weight.vec = LMLogisticLossIterations(X.mat, y.vec,selected.steps,step.size)
#try to get the train.loss.vec(I don't know what does training loss mean)
#predict(testX.mat)
testX.mat = data.matrix(TestingData) %*% data.matrix(W.Matrix)
#return list
list=list(mean.validation.loss,selected.steps,weight.vec,testX.mat)
return (list)
}
LMSquareLossIterations <-
function(X.mat, y.vec, max.iterations, step.size = 0.001) {
if (!all(is.matrix(X.mat), is.numeric(X.mat))) {
stop("X.mat must be a numeric matrix.")
}
if (!all(is.vector(y.vec),
is.numeric(y.vec),
length(y.vec) == nrow(X.mat))) {
stop("y.vec must be a numeric vector of the same number of rows as X.mat.")
}
if (!all(is.integer(max.iterations),
max.iterations > 1,
length(max.iterations) == 1)) {
stop("Input max.iterations must be a greater than 1 integer scalar number.")
}
if (!all(is.numeric(step.size), length(step.size) == 1)) {
stop("step.size must be a numeric scalar value.")
}
num.train <- dim(X.mat)[1]
num.feature <- dim(X.mat)[2]
W=matrix(data=0,ncol=1,nrow=num.feature)
W.mat=matrix(0,ncol=max.iterations,nrow=num.feature)
X.scale.vec <- scale(X.mat,center=TRUE,scale = TRUE)[1:num.train,1:num.feature]#variance=1,mean=0
# for-loop to get the slope.mat matrix
for (iter.index in (1:max.iterations)) {
grad <- t(X.scale.vec) %*% (X.scale.vec %*% W -  y.vec);
if (sqrt(as.numeric(t(grad) %*% grad)) < 1e-6){
println(sprintf('iter times=%d', iter.index));
break;
}
W <- W - grad * step.size;
W.mat[,iter.index]=W
}
return(W.mat)
}
LMLogisticLossIterations <-
function(X.mat, y.vec,max.iterations, step.size = 0.05) {
if (!all(is.matrix(X.mat), is.numeric(X.mat))) {
stop("X.mat must be a numeric matrix.")
}
if (!all(is.vector(y.vec),
is.numeric(y.vec),
length(y.vec) == nrow(X.mat))) {
stop("y.vec must be a numeric vector of the same number of rows as X.mat.")
}
if (!all(is.integer(max.iterations),
max.iterations > 1,
length(max.iterations) == 1)) {
stop("Input max.iterations must be a greater than 1 integer scalar number.")
}
if (!all(is.numeric(step.size), length(step.size) == 1)) {
stop("step.size must be a numeric scalar value.")
}
num.train <- dim(X.mat)[1]
num.feature <- dim(X.mat)[2]
W=matrix(data=0,ncol=1,nrow=num.feature)
sigmoid <- function(z) { 1 / (1 + exp(-z))}
X.scale.vec <- scale(X.mat,center=TRUE,scale = TRUE)[1:num.train,1:num.feature]#variance=1,mean=0
W.mat=matrix(0,ncol=max.iterations,nrow=num.feature)
for (i in 1:max.iterations){
grad <- t(X.scale.vec) %*% (sigmoid(X.scale.vec %*% W)-y.vec);
if (sqrt(as.numeric(t(grad) %*% grad)) < 1e-6){
print(sprintf('iter times=%d', i));
break;
}
W <- W - grad * step.size;
W.mat[,i]=W
}
return(W.mat)
}
LMLogisticLossEarlyStoppingCV<-function(X.mat, y.vec, fold.vec,folds.n=2,max.iterations)
{
step.size = 0.01
#give the initial step size
Data = cbind(X.mat ,y.vec)
#Randomly shuffle the data
Data<-Data[sample(nrow(Data)),]
#split the data into several folds
Start = 0
End   = NCOL(Data) - 1
LabelCol      = NCOL(Data)
Rows          = NROW(Data)
#initialize Error Matrix:
L2Error.Matrix = 0
#Perform folds.n fold cross validation
for(i in 1:folds.n){
#Segement your data by fold using the which() function
positions <- which(fold.vec == i)
testdata <- Data[positions, ]
traindata <- Data[-positions, ]
test.data    <- testdata[,DataStart:DataEnd]
train.labels  <- testdata[,LabelCol]
train.data   <- trainData[,Start:End]
train.labels <- trainData[,LabelCol]
#use LMLogisticLossIterations to compute a sequence of models
W= LMSquareLossIterations(train.data,train.labels,max.iterations,step.size)
#on the train data, then compute the validation loss of each model.
Error = 0
Y.sigmoid = 1/(1+exp(-data.matrix(TestingData) %*% data.matrix(W.Matrix)))
Error_Vector = ifelse(Y.sigmoid>0.5,1,0) != as.integer(TestingLables)
Error = (as.matrix(colMeans(Error_Vector)))
LMLogisticLoss.Matrix   <- rbind( LMLogisticLoss.Matrix,t(Error))
}
#compute mean.validation.loss.vec, which is a vector (with max.iterations elements) of mean validation loss over all K folds.
mean.validation.loss = colMeans( LMLogisticLoss.Matrix)
#minimize the mean validation loss to determine selected.steps, the optimal number of steps/iterations.
SmallestLoss = max(a)
selected.steps = 0
for(i in 1:max.iterations)
{
if(mean.validation.loss[i] < SmallestLoss)
{
SmallestLoss = mean.validation.loss[i]
selected.steps        = i
}
}
#finally use LMLinearSquareLossIterations(max.iterations=selected.steps) on the whole training data set.
weight.vec = LMLogisticLossIterations(X.mat, y.vec,selected.steps,step.size)
#try to get the train.loss.vec(I don't know what does training loss mean)
#predict(testX.mat)
Y.sigmoid = 1/(1+exp(-data.matrix(X.mat) %*% data.matrix(weight.vec)))
testX.mat=ifelse(Y.sigmoid>0.5,1,0) != as.integer(TestingLables)
#return list
list=list(mean.validation.loss,selected.steps,weight.vec,testX.mat)
return (list)
}
LMLogisticLossEarlyStoppingCV<-function(X.mat, y.vec, fold.vec,folds.n=2,max.iterations)
{
step.size = 0.01#give the initial step size
Data = cbind(X.mat ,y.vec)
#Randomly shuffle the data
Data<-Data[sample(nrow(Data)),]
#TODO If invalid fold.vec .. make one for ourselves
Start = 0
End   = NCOL(Data) - 1
LabelCol      = NCOL(Data)
Rows          = NROW(Data)
#initialize Error Matrix:
L2Error.Matrix = 0
#Perform folds.n fold cross validation
for(i in 1:folds.n){
#Segement your data by fold using the which() function
positions <- which(fold.vec == i)
testdata <- Data[positions, ]
traindata <- Data[-positions, ]
test.data    <- testdata[,Start:End]
train.labels  <- testdata[,LabelCol]
train.data   <- trainData[,DataColsStart:DataColsEnd]
train.labels <- trainData[,LabelCol]
#use LMLogisticLossIterations to compute a sequence of models
W= LMSquareLossIterations(train.data,train.labels,max.iterations,step.size)
#on the train data, then compute the validation loss of each model.
Error = 0
Y.square = data.matrix(TestingData) %*% data.matrix(W)
Error_Vector = ((Y.square) - as.integer(TestingLables))**2
Error = (as.matrix(colMeans(Error_Vector)))
LMLogisticLoss.Matrix   <- rbind( LMLogisticLoss.Matrix,t(Error))
}
#compute mean.validation.loss.vec, which is a vector (with max.iterations elements) of mean validation loss over all K folds.
mean.validation.loss = colMeans( LMLogisticLoss.Matrix)
#minimize the mean validation loss to determine selected.steps, the optimal number of steps/iterations.
SmallestLoss = max(a)
selected.steps = 0
for(i in 1:max.iterations)
{
if(mean.validation.loss[i] < SmallestLoss)
{
SmallestLoss = mean.validation.loss[i]
selected.steps        = i
}
}
#finally use LMLinearSquareLossIterations(max.iterations=selected.steps) on the whole training data set.
weight.vec = LMLogisticLossIterations(X.mat, y.vec,selected.steps,step.size)
#try to get the train.loss.vec(I don't know what does training loss mean)
#predict(testX.mat)
testX.mat = data.matrix(TestingData) %*% data.matrix(W.Matrix)
#return list
list=list(mean.validation.loss,selected.steps,weight.vec,testX.mat)
return (list)
}
